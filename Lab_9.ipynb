{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJAWelEtvZPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "    return np.exp(z) / np.sum(np.exp(z))\n",
        "\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "\n",
        "def tanh_prime(z):\n",
        "    return 1 - tanh(z) * tanh(z)\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "\n",
        "def relu_prime(z):\n",
        "    return z > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TKCwyPtvkex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sizes=[784, 30, 10],\n",
        "        learning_rate=1e-2,\n",
        "        mini_batch_size=16,\n",
        "        activation_fn=\"relu\"\n",
        "    ):\n",
        "        \n",
        "        self.sizes = sizes\n",
        "        self.num_layers = len(sizes)\n",
        "        self.activation_fn = getattr(activations, activation_fn)\n",
        "        self.activation_fn_prime = getattr(activations, f\"{activation_fn}_prime\")\n",
        "\n",
        "        \n",
        "        self.weights = [np.array([0])] + [np.random.randn(y, x)/np.sqrt(x) for y, x in\n",
        "                                          zip(sizes[1:], sizes[:-1])]\n",
        "\n",
        "        \n",
        "        self.biases = [np.array([0])] + [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "\n",
        "        self._zs = [np.zeros(bias.shape) for bias in self.biases]\n",
        "\n",
        "      \n",
        "        self._activations = [np.zeros(bias.shape) for bias in self.biases]\n",
        "\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def fit(self, training_data, validation_data=None, epochs=10):\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k + self.mini_batch_size] for k in\n",
        "                range(0, len(training_data), self.mini_batch_size)]\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
        "                nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
        "                for x, y in mini_batch:\n",
        "                    self._forward_prop(x)\n",
        "                    delta_nabla_b, delta_nabla_w = self._back_prop(x, y)\n",
        "                    nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "                    nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "                self.weights = [\n",
        "                    w - (self.lr / self.mini_batch_size) * dw for w, dw in\n",
        "                    zip(self.weights, nabla_w)]\n",
        "                self.biases = [\n",
        "                    b - (self.lr / self.mini_batch_size) * db for b, db in\n",
        "                    zip(self.biases, nabla_b)]\n",
        "\n",
        "            if validation_data:\n",
        "                accuracy = self.validate(validation_data) / 100.0\n",
        "                print(f\"Epoch {epoch + 1}, accuracy {accuracy} %.\")\n",
        "            else:\n",
        "                print(f\"Processed epoch {epoch}.\")\n",
        "\n",
        "    def validate(self, validation_data):\n",
        "        \n",
        "        validation_results = [(self.predict(x) == y) for x, y in validation_data]\n",
        "        return sum(result for result in validation_results)\n",
        "\n",
        "    def predict(self, x):\n",
        "       \n",
        "\n",
        "        self._forward_prop(x)\n",
        "        return np.argmax(self._activations[-1])\n",
        "\n",
        "    def _forward_prop(self, x):\n",
        "        self._activations[0] = x\n",
        "        for i in range(1, self.num_layers):\n",
        "            self._zs[i] = (\n",
        "                self.weights[i].dot(self._activations[i - 1]) + self.biases[i]\n",
        "            )\n",
        "           \n",
        "            if i == self.num_layers - 1:\n",
        "                self._activations[i] = activations.softmax(self._zs[i])\n",
        "            else:\n",
        "                self._activations[i] = self.activation_fn(self._zs[i])\n",
        "\n",
        "    def _back_prop(self, x, y):\n",
        "        nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
        "        nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
        "\n",
        "        error = (self._activations[-1] - y)\n",
        "        nabla_b[-1] = error\n",
        "        nabla_w[-1] = error.dot(self._activations[-2].transpose())\n",
        "\n",
        "        for l in range(self.num_layers - 2, 0, -1):\n",
        "            error = np.multiply(\n",
        "                self.weights[l + 1].transpose().dot(error),\n",
        "                self.activation_fn_prime(self._zs[l])\n",
        "            )\n",
        "            nabla_b[l] = error\n",
        "            nabla_w[l] = error.dot(self._activations[l - 1].transpose())\n",
        "\n",
        "        return nabla_b, nabla_w\n",
        "\n",
        "    def load(self, filename='model.npz'):\n",
        "     \n",
        "        npz_members = np.load(os.path.join(os.curdir, 'models', filename))\n",
        "\n",
        "        self.weights = list(npz_members['weights'])\n",
        "        self.biases = list(npz_members['biases'])\n",
        "\n",
        "     \n",
        "        self.sizes = [b.shape[0] for b in self.biases]\n",
        "        self.num_layers = len(self.sizes)\n",
        "\n",
        "        self._zs = [np.zeros(bias.shape) for bias in self.biases]\n",
        "        self._activations = [np.zeros(bias.shape) for bias in self.biases]\n",
        "\n",
        "       \n",
        "        self.mini_batch_size = int(npz_members['mini_batch_size'])\n",
        "        self.lr = float(npz_members['lr'])\n",
        "\n",
        "    def save(self, filename='model.npz'):\n",
        "       \n",
        "        np.savez_compressed(\n",
        "            file=os.path.join(os.curdir, 'models', filename),\n",
        "            weights=self.weights,\n",
        "            biases=self.biases,\n",
        "            mini_batch_size=self.mini_batch_size,\n",
        "            lr=self.lr\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biEhenbXwYkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import wget\n",
        "import numpy as np\n",
        "def load_mnist():\n",
        "    if not os.path.exists(os.path.join(os.curdir, \"data\")):\n",
        "        os.mkdir(os.path.join(os.curdir, \"data\"))\n",
        "        wget.download(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", out=\"data\")\n",
        "\n",
        "    data_file = gzip.open(os.path.join(os.curdir, \"data\", \"mnist.pkl.gz\"), \"rb\")\n",
        "    train_data, val_data, test_data = pickle.load(data_file, encoding=\"latin1\")\n",
        "    data_file.close()\n",
        "\n",
        "    train_inputs = [np.reshape(x, (784, 1)) for x in train_data[0]]\n",
        "    train_results = [vectorized_result(y) for y in train_data[1]]\n",
        "    train_data = list(zip(train_inputs, train_results))\n",
        "\n",
        "    val_inputs = [np.reshape(x, (784, 1)) for x in val_data[0]]\n",
        "    val_results = val_data[1]\n",
        "    val_data = list(zip(val_inputs, val_results))\n",
        "\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
        "    test_data = list(zip(test_inputs, test_data[1]))\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "def vectorized_result(y):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[y] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    layers = [784, 30, 10]\n",
        "    learning_rate = 0.01\n",
        "    mini_batch_size = 16\n",
        "    epochs = 100\n",
        "\n",
        "    # Initialize train, val and test data\n",
        "    train_data, val_data, test_data = load_mnist()\n",
        "\n",
        "    nn = NeuralNetwork(layers, learning_rate, mini_batch_size, \"relu\")\n",
        "    nn.fit(train_data, val_data, epochs)\n",
        "\n",
        "    accuracy = nn.validate(test_data) / 100.0\n",
        "    print(f\"Test Accuracy: {accuracy}%.\")\n",
        "\n",
        "    nn.save()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}